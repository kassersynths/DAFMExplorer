{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" style=\"background: #ffffff; padding: 40px; margin-bottom: 10px;\">\n",
        "\n",
        "<!-- Kasser Synths Logo -->\n",
        "<img src=\"images/logo-kasser-synths.svg\" alt=\"Kasser Synths Logo\" style=\"width: 400px; max-width: 90%; height: auto; display: block; margin-left: auto; margin-right: auto;\" />\n",
        "\n",
        "</div>\n",
        "\n",
        "<div align=\"center\" style=\"background: linear-gradient(135deg, #000000 0%, #1a1a1a 100%); padding: 20px; border: 2px solid #00d4ff; margin-bottom: 30px;\">\n",
        "\n",
        "<div style=\"font-size: 14px; color: #00d4ff; letter-spacing: 3px;\">\n",
        "DATA EXTRACTION ‚Ä¢ WEB SCRAPING ‚Ä¢ PREPROCESSING\n",
        "</div>\n",
        "\n",
        "</div>\n",
        "\n",
        "# üì¶ Data Extraction Notebook\n",
        "\n",
        "## From Chips to Dataframes: The Complete Data Pipeline\n",
        "\n",
        "This notebook is the **foundation** of our analysis. Here, we extract raw data from multiple sources, transform it into structured formats, and prepare it for machine learning. Every step matters.\n",
        "\n",
        "**What You'll Learn:**\n",
        "- How VGM files \"spy\" on sound chips to capture every command\n",
        "- The preservation projects that saved gaming history\n",
        "- Why data quality matters: duplicates, validation, and cleaning\n",
        "- Web scraping techniques for enriching datasets\n",
        "- The complete data pipeline: from files to analysis-ready data\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Important Note on Caching\n",
        "\n",
        "**This notebook implements automatic caching.** If output files already exist, processing/scraping will NOT run again.\n",
        "\n",
        "**Generated files**:\n",
        "- `data/processed/all_instruments_final.csv` - Raw presets from OPM files\n",
        "- `artifacts/presets_cleaned.csv` - Cleaned presets (deduplicated per game)\n",
        "- `artifacts/gems_games.csv` - GEMS game list\n",
        "- `artifacts/vgmrips_composers.csv` - Game-composer mappings\n",
        "- `artifacts/composers_info.csv` - Composer info (nationality)\n",
        "\n",
        "To **force a re-process**, manually delete the corresponding files.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART I: THE DATA SOURCES - Understanding What We're Extracting\n",
        "\n",
        "## 1. Introduction: The Mission\n",
        "\n",
        "### 1.1 Purpose of This Notebook\n",
        "\n",
        "This notebook is where **raw data becomes structured data**. It's the first step in our data science pipeline, and arguably the most important. Without proper data extraction and cleaning, all subsequent analysis would be meaningless.\n",
        "\n",
        "**What We're Building:**\n",
        "- A complete dataset of 93,000+ FM presets from Sega Genesis games\n",
        "- Enriched metadata: game names, composers, nationalities\n",
        "- Clean, validated data ready for machine learning\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Data Quality**: Garbage in, garbage out. Clean extraction = reliable analysis\n",
        "- **Reproducibility**: Every step is documented and cached\n",
        "- **Scalability**: Our methods can handle large datasets efficiently\n",
        "\n",
        "### 1.2 The Data Pipeline: From Chips to Analysis\n",
        "\n",
        "```\n",
        "Sound Chip (YM2612) \n",
        "    ‚Üì\n",
        "VGM Files (Recorded Commands)\n",
        "    ‚Üì\n",
        "OPM Files (Preset Format)\n",
        "    ‚Üì\n",
        "CSV Files (Structured Data)\n",
        "    ‚Üì\n",
        "Cleaned Dataset (Ready for ML)\n",
        "    ‚Üì\n",
        "Analysis & Insights\n",
        "```\n",
        "\n",
        "**Each step transforms data into a more useful format.**\n",
        "\n",
        "### 1.3 Why Data Extraction Matters\n",
        "\n",
        "In data science, **80% of the work is data preparation**. This notebook does that work:\n",
        "\n",
        "- **Extraction**: Getting data from multiple sources (files, websites, APIs)\n",
        "- **Transformation**: Converting formats, parsing text, extracting features\n",
        "- **Validation**: Ensuring data quality and consistency\n",
        "- **Enrichment**: Adding context (composers, game info, metadata)\n",
        "\n",
        "**The Result**: A dataset that tells a complete story, not just numbers.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. VGM Files: The Digital Spies\n",
        "\n",
        "### 2.1 What Are VGM Files?\n",
        "\n",
        "**VGM** (Video Game Music) files are recordings of **exact chip commands** sent to sound chips during gameplay. Think of them as \"spy recordings\" that capture every register write, every parameter change, every note played.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![VGM File Example](images/vgm-file-example.png)\n",
        "\n",
        "*Example of a VGM file structure - recording every chip command*  \n",
        "*Image: VGM file format documentation*\n",
        "\n",
        "</div>\n",
        "\n",
        "**Key Characteristics:**\n",
        "- **Lossless**: Every command is recorded exactly as sent\n",
        "- **Replayable**: Can be played back to recreate the exact sound\n",
        "- **Preservable**: Captures music that would otherwise be lost\n",
        "- **Analyzable**: Contains all the data we need for analysis\n",
        "\n",
        "### 2.2 How VGM Files Capture Chip Commands (The \"Spy\" Analogy)\n",
        "\n",
        "Imagine a **spy** sitting inside the Sega Genesis, watching every command sent to the YM2612 chip:\n",
        "\n",
        "1. **The Game Sends a Command**: \"Set operator 1's attack rate to 31\"\n",
        "2. **The Spy Records It**: Writes it to the VGM file with a timestamp\n",
        "3. **The Chip Executes It**: The sound changes\n",
        "4. **Repeat**: Every single command is captured\n",
        "\n",
        "**The Recording Process:**\n",
        "- VGM files intercept **register writes** to the sound chip\n",
        "- Each write is recorded with its **address** (which register) and **value** (what data)\n",
        "- The file also records **timing** (when each command occurred)\n",
        "- This creates a **complete log** of all sound chip activity\n",
        "\n",
        "**Why This Is Powerful:**\n",
        "- We can **extract presets** from VGM files\n",
        "- We can **analyze patterns** in how composers used the chip\n",
        "- We can **recreate sounds** exactly as they were in the game\n",
        "- We can **preserve history** that would otherwise be lost\n",
        "\n",
        "### 2.3 The Recording Process: Capturing Every Register Write\n",
        "\n",
        "The YM2612 chip has **hundreds of registers** that control every aspect of sound:\n",
        "\n",
        "| Register Type | Purpose | Example |\n",
        "|---------------|---------|---------|\n",
        "| **Algorithm** | How operators connect | Register 0xB0: CON = 4 |\n",
        "| **Feedback** | Self-modulation | Register 0xB0: FL = 7 |\n",
        "| **Envelope** | Attack, decay, sustain | Register 0x30: AR = 31 |\n",
        "| **Frequency** | Pitch of the note | Register 0xA4: F-Number |\n",
        "| **Total Level** | Volume | Register 0x40: TL = 0 |\n",
        "\n",
        "**VGM files record ALL of these writes**, creating a complete snapshot of the chip's state at every moment.\n",
        "\n",
        "### 2.4 VGM File Structure and Format\n",
        "\n",
        "VGM files have a specific structure:\n",
        "\n",
        "```\n",
        "Header (Metadata)\n",
        "    ‚Üì\n",
        "Command Stream (Register Writes)\n",
        "    ‚Üì\n",
        "End Marker\n",
        "```\n",
        "\n",
        "**The Header Contains:**\n",
        "- Game name\n",
        "- System (Genesis/Mega Drive)\n",
        "- Chip type (YM2612)\n",
        "- Recording date\n",
        "- Loop points\n",
        "\n",
        "**The Command Stream Contains:**\n",
        "- Register addresses (which register to write to)\n",
        "- Register values (what data to write)\n",
        "- Timing information (when to execute)\n",
        "\n",
        "### 2.5 Why VGM Files Are Perfect for Analysis\n",
        "\n",
        "VGM files are ideal for data science because:\n",
        "\n",
        "1. **Complete Data**: Every parameter is recorded\n",
        "2. **Structured Format**: Can be parsed programmatically\n",
        "3. **Preserved History**: Games from 30+ years ago are still accessible\n",
        "4. **Reproducible**: Can extract the same data multiple times\n",
        "5. **Rich Metadata**: Game names, dates, and other context included\n",
        "\n",
        "**The Challenge**: VGM files are **binary** and need special tools to parse. That's where OPM files come in.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. The Preservation Projects: VGMrips and Project2612\n",
        "\n",
        "### 3.1 Project2612: The Complete Collection\n",
        "\n",
        "**Project2612** was a massive preservation effort to record **every YM2612 game** in VGM format. Named after the chip itself, it represents one of the most comprehensive collections of video game music ever assembled.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Project2612 Logo](images/project2612-logo.png)\n",
        "\n",
        "*Project2612 - Preserving the complete YM2612 library*  \n",
        "*Image: Project2612 website*\n",
        "\n",
        "</div>\n",
        "\n",
        "**What Project2612 Did:**\n",
        "- Recorded VGM files from **hundreds of games**\n",
        "- Created a standardized format for preservation\n",
        "- Made the collection available to the community\n",
        "- Documented every game with metadata\n",
        "\n",
        "**The DrWashington Collection:**\n",
        "- A complete dump of Project2612 data up to 2010\n",
        "- Contains **93,000+ presets** extracted from VGM files\n",
        "- Our primary data source for this analysis\n",
        "- Found on the KVRist forum as a treasure trove of creative work\n",
        "\n",
        "### 3.2 VGMrips: The Community Database\n",
        "\n",
        "**VGMrips** (vgmrips.net) is a community-maintained database of video game music. Unlike Project2612, which focused on complete preservation, VGMrips focuses on **organization and metadata**.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![VGMrips Website](images/vgmrips-website.png)\n",
        "\n",
        "*VGMrips - The community database of game music*  \n",
        "*Image: vgmrips.net*\n",
        "\n",
        "</div>\n",
        "\n",
        "**What VGMrips Provides:**\n",
        "- **Game-composer mappings**: Who composed what\n",
        "- **Organized packs**: Games grouped by chip, system, or composer\n",
        "- **Metadata**: Release dates, regions, additional information\n",
        "- **Community contributions**: Users can add and correct information\n",
        "\n",
        "**Why We Use VGMrips:**\n",
        "- **Composer Information**: Essential for understanding creative context\n",
        "- **Game Identification**: Helps match games across different sources\n",
        "- **Quality Control**: Community-verified data is more reliable\n",
        "\n",
        "### 3.3 How These Projects Preserved Gaming History\n",
        "\n",
        "Before VGM files and preservation projects, game music was **trapped** in the hardware:\n",
        "\n",
        "- **No way to extract**: Music was embedded in ROM chips\n",
        "- **No way to preserve**: Original hardware degrades over time\n",
        "- **No way to analyze**: Couldn't study the music without playing the game\n",
        "\n",
        "**The Preservation Revolution:**\n",
        "1. **VGM Format**: Created a way to record chip commands\n",
        "2. **Recording Tools**: Software to capture VGM files from emulators\n",
        "3. **Community Effort**: Thousands of volunteers recording games\n",
        "4. **Organization**: Databases like VGMrips organizing the data\n",
        "5. **Accessibility**: Making preserved music available to everyone\n",
        "\n",
        "**The Result**: We can now analyze music from games that are 30+ years old, extract presets, study composer techniques, and preserve this cultural heritage.\n",
        "\n",
        "### 3.4 The DrWashington Collection: Our Dataset Source\n",
        "\n",
        "The **DrWashington collection** is a complete archive of Project2612 data, including:\n",
        "\n",
        "- **VGM Files**: Recordings of chip commands\n",
        "- **OPM Files**: Extracted presets in OPM format\n",
        "- **Metadata**: Game names, dates, and other information\n",
        "- **Complete Coverage**: Every YM2612 game up to 2010\n",
        "\n",
        "**Why This Collection Is Valuable:**\n",
        "- **Completeness**: Contains nearly every Genesis game\n",
        "- **Consistency**: All files in the same format\n",
        "- **Extracted Presets**: OPM files already extracted from VGM\n",
        "- **Historical Snapshot**: Captures the state of preservation in 2010\n",
        "\n",
        "**Our Dataset**: We use the OPM files from this collection, which contain **93,000+ presets** ready for analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Preset Formats: OPM and DMP\n",
        "\n",
        "### 4.1 OPM Format: The Original Preset Format\n",
        "\n",
        "**OPM** (Operator Music) is a text-based format for storing FM synthesis presets. It was used by early tools and is the format we extract from VGM files.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![OPM File Example](images/opm-file-example.png)\n",
        "\n",
        "*Example of an OPM file - text-based preset format*  \n",
        "*Image: OPM format documentation*\n",
        "\n",
        "</div>\n",
        "\n",
        "**OPM File Structure:**\n",
        "```\n",
        "@:0 Preset Name\n",
        "LFO: 0 0 0 0 0\n",
        "CH: 0 0 0 0 0 0 0\n",
        "M1: 31 0 0 0 0 0 0 0 0 0 0\n",
        "C1: 31 0 0 0 0 0 0 0 0 0 0\n",
        "M2: 31 0 0 0 0 0 0 0 0 0 0\n",
        "C2: 31 0 0 0 0 0 0 0 0 0 0\n",
        "```\n",
        "\n",
        "**What OPM Contains:**\n",
        "- **Preset Name**: Human-readable identifier\n",
        "- **LFO Parameters**: Low-frequency oscillator settings\n",
        "- **Channel Parameters**: Algorithm, feedback, panning\n",
        "- **Operator Parameters**: 4 operators √ó 11 parameters each = 44 parameters\n",
        "- **Total**: 58 parameters per preset\n",
        "\n",
        "### 4.2 DMP Format: DefleMask's Modern Format\n",
        "\n",
        "**DMP** (DefleMask Preset) is the modern format used by DefleMask, a popular tracker for retro sound chips. It's more structured than OPM and includes additional metadata.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![DefleMask Interface](images/deflemask-interface.png)\n",
        "\n",
        "*DefleMask - Modern tool for creating FM presets*  \n",
        "*Image: deflemask.com*\n",
        "\n",
        "</div>\n",
        "\n",
        "**DMP Advantages:**\n",
        "- **Structured Format**: Easier to parse programmatically\n",
        "- **Metadata**: Includes additional information\n",
        "- **Modern Tools**: Better support in current software\n",
        "- **Compatibility**: Can be converted to/from OPM\n",
        "\n",
        "**Why We Use OPM:**\n",
        "- **Historical Accuracy**: OPM is what was extracted from VGM files\n",
        "- **Completeness**: Our dataset is in OPM format\n",
        "- **Compatibility**: Can be converted to DMP if needed\n",
        "\n",
        "### 4.3 Why We Extract from OPM Files\n",
        "\n",
        "OPM files are our **primary source** because:\n",
        "\n",
        "1. **Direct Extraction**: OPM files were extracted from VGM files\n",
        "2. **Complete Data**: Contains all 58 parameters per preset\n",
        "3. **Text-Based**: Easy to parse with regular expressions\n",
        "4. **Historical Format**: Preserves the original extraction format\n",
        "5. **Large Dataset**: 93,000+ presets already in OPM format\n",
        "\n",
        "**The Extraction Process:**\n",
        "1. VGM files contain register writes\n",
        "2. Tools extract presets from VGM files\n",
        "3. Presets are saved in OPM format\n",
        "4. We parse OPM files to extract parameters\n",
        "5. Parameters are stored in CSV for analysis\n",
        "\n",
        "### 4.4 The Parameter Structure: 58 Dimensions of Sound\n",
        "\n",
        "Each OPM preset contains **58 parameters**:\n",
        "\n",
        "| Category | Parameters | Description |\n",
        "|----------|------------|-------------|\n",
        "| **LFO** | 5 | Low-frequency oscillator settings |\n",
        "| **Channel** | 7 | Algorithm, feedback, panning, etc. |\n",
        "| **Operator 1 (M1)** | 11 | Attack, decay, sustain, release, etc. |\n",
        "| **Operator 2 (C1)** | 11 | Carrier 1 parameters |\n",
        "| **Operator 3 (M2)** | 11 | Modulator 2 parameters |\n",
        "| **Operator 4 (C2)** | 11 | Carrier 2 parameters |\n",
        "| **Total** | **58** | Complete preset configuration |\n",
        "\n",
        "**Why 58 Dimensions Matter:**\n",
        "- **High-Dimensional Data**: Requires dimensionality reduction (PCA, t-SNE)\n",
        "- **Rich Information**: Every parameter affects the sound\n",
        "- **Pattern Discovery**: ML can find patterns in 58D space\n",
        "- **Creative Analysis**: Can study how composers used parameters\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Setup and Imports\n",
        "\n",
        "Before we begin extraction, we need to set up our environment and import necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# üåê GOOGLE COLAB SETUP\n",
        "# ============================================================\n",
        "# This cell automatically sets up the environment for Google Colab.\n",
        "# If running locally, it does nothing.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üåê Running in Google Colab - Setting up environment...\")\n",
        "    \n",
        "    # Clone the repository if not already cloned\n",
        "    REPO_URL = \"https://github.com/kassersynths/DAFMExplorer.git\"\n",
        "    REPO_DIR = \"/content/DAFMExplorer\"\n",
        "    \n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        print(f\"   üì• Cloning repository from {REPO_URL}...\")\n",
        "        !git clone {REPO_URL} {REPO_DIR}\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Repository already exists at {REPO_DIR}\")\n",
        "        # Pull latest changes\n",
        "        !cd {REPO_DIR} && git pull\n",
        "    \n",
        "    # Change to the repository directory\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"   üìÇ Working directory: {os.getcwd()}\")\n",
        "    print(\"   ‚úÖ Colab setup complete!\")\n",
        "else:\n",
        "    print(\"üíª Running locally - no setup needed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "import time\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART II: DATA EXTRACTION - From Files to Dataframes\n",
        "\n",
        "## 5. OPM Preset Extraction\n",
        "\n",
        "### 5.1 Parsing OPM Files: The Challenge\n",
        "\n",
        "OPM files are **text-based**, which makes them easier to parse than binary formats, but they still present challenges:\n",
        "\n",
        "- **Irregular Structure**: Not all files follow the exact same format\n",
        "- **Missing Data**: Some presets may have incomplete parameter sets\n",
        "- **Encoding Issues**: Special characters in preset names\n",
        "- **Large Volume**: 93,000+ files to process\n",
        "\n",
        "**Our Approach:**\n",
        "- Use **regular expressions** to find patterns\n",
        "- Handle edge cases gracefully\n",
        "- Validate data as we extract it\n",
        "- Process files in batches for efficiency\n",
        "\n",
        "### 5.2 Regular Expressions: Finding Patterns in Text\n",
        "\n",
        "Regular expressions (regex) are powerful tools for extracting structured data from text. We use them to find:\n",
        "\n",
        "- **Preset Names**: `@:0 Preset Name`\n",
        "- **LFO Parameters**: `LFO: 0 0 0 0 0`\n",
        "- **Channel Parameters**: `CH: 0 0 0 0 0 0 0`\n",
        "- **Operator Parameters**: `M1: 31 0 0 0 0 0 0 0 0 0 0`\n",
        "\n",
        "**Why Regex Works:**\n",
        "- **Pattern Matching**: Finds data even with variations\n",
        "- **Efficient**: Fast processing of large text files\n",
        "- **Flexible**: Can handle different file formats\n",
        "- **Reliable**: Consistent extraction across all files\n",
        "\n",
        "### 5.3 Extracting Parameters: Operators, Algorithms, Envelopes\n",
        "\n",
        "Each OPM preset contains multiple parameter groups:\n",
        "\n",
        "1. **LFO Parameters**: Control modulation effects\n",
        "2. **Channel Parameters**: Algorithm, feedback, panning\n",
        "3. **Operator Parameters**: 4 operators √ó 11 parameters each\n",
        "\n",
        "**The Extraction Process:**\n",
        "1. Read each OPM file line by line\n",
        "2. Match patterns using regex\n",
        "3. Extract parameter values\n",
        "4. Combine into a structured dataframe\n",
        "5. Validate data completeness\n",
        "\n",
        "### 5.4 Handling Edge Cases: Malformed Files, Missing Data\n",
        "\n",
        "Not all OPM files are perfect. We handle:\n",
        "\n",
        "- **Missing Parameters**: Use default values or skip incomplete presets\n",
        "- **Malformed Lines**: Skip lines that don't match expected patterns\n",
        "- **Encoding Errors**: Handle special characters gracefully\n",
        "- **Empty Files**: Skip files with no valid presets\n",
        "\n",
        "**Quality Control:**\n",
        "- Log warnings for problematic files\n",
        "- Validate parameter ranges (0-127, 0-7, etc.)\n",
        "- Check for required parameters\n",
        "- Ensure data consistency\n",
        "\n",
        "### 5.5 Output: The Raw Preset Dataset\n",
        "\n",
        "After extraction, we have:\n",
        "\n",
        "- **93,000+ presets** in structured format\n",
        "- **58 parameters** per preset\n",
        "- **Game names** extracted from filenames\n",
        "- **Preset names** from OPM files\n",
        "\n",
        "**The Raw Dataset:**\n",
        "- `data/processed/all_instruments_final.csv`\n",
        "- Ready for further processing\n",
        "- Contains all extracted parameters\n",
        "- Includes metadata (filename, preset name)\n",
        "\n",
        "**Input**: `data/raw/OPM presets.zip`  \n",
        "**Output**: `data/processed/all_instruments_final.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define regular expressions for each parameter\n",
        "name_regex = re.compile(r\"@:(\\d+)\\s(.+)\")\n",
        "lfo_regex = re.compile(r\"LFO:\\s(.+)\")\n",
        "ch_regex = re.compile(r\"CH:\\s(.+)\")\n",
        "m1_regex = re.compile(r\"M1:\\s(.+)\")\n",
        "c1_regex = re.compile(r\"C1:\\s(.+)\")\n",
        "m2_regex = re.compile(r\"M2:\\s(.+)\")\n",
        "c2_regex = re.compile(r\"C2:\\s(.+)\")\n",
        "\n",
        "def parse_opm(file_contents, filename):\n",
        "    \"\"\"Parse an OPM file and extract all preset parameters.\"\"\"\n",
        "    names, lfos, chs, m1s, c1s, m2s, c2s = [], [], [], [], [], [], []\n",
        "    current_instrument_name = ''\n",
        "    lfo_match_index = 0\n",
        "    ch_match_index = 0\n",
        "\n",
        "    for line in file_contents.split('\\n'):\n",
        "        name_match = name_regex.search(line)\n",
        "        if name_match:\n",
        "            current_instrument_name = name_match.group(2).strip()\n",
        "            if current_instrument_name.lower() != 'no name':\n",
        "                names.append([int(name_match.group(1)), current_instrument_name])\n",
        "        if current_instrument_name.lower() != 'no name':\n",
        "            lfo_match = lfo_regex.search(line)\n",
        "            if lfo_match:\n",
        "                if lfo_match_index != 0:\n",
        "                    lfos.append([int(n) if n.isdigit() else n for n in lfo_match.group(1).split()])\n",
        "                lfo_match_index += 1\n",
        "            ch_match = ch_regex.search(line)\n",
        "            if ch_match:\n",
        "                if ch_match_index != 0:\n",
        "                    chs.append([int(n) if n.isdigit() else n for n in ch_match.group(1).split()])\n",
        "                ch_match_index += 1\n",
        "            m1_match = m1_regex.search(line)\n",
        "            if m1_match:\n",
        "                m1s.append([int(n) if n.isdigit() else n for n in m1_match.group(1).split()])\n",
        "            c1_match = c1_regex.search(line)\n",
        "            if c1_match:\n",
        "                c1s.append([int(n) if n.isdigit() else n for n in c1_match.group(1).split()])\n",
        "            m2_match = m2_regex.search(line)\n",
        "            if m2_match:\n",
        "                m2s.append([int(n) if n.isdigit() else n for n in m2_match.group(1).split()])\n",
        "            c2_match = c2_regex.search(line)\n",
        "            if c2_match:\n",
        "                c2s.append([int(n) if n.isdigit() else n for n in c2_match.group(1).split()])\n",
        "\n",
        "    # Create dataframes\n",
        "    names_df = pd.DataFrame(names, columns=['Num', 'Name'])\n",
        "    lfos_df = pd.DataFrame(lfos, columns=['LFRQ', 'AMD', 'PMD', 'WF', 'NFRQ'])\n",
        "    chs_df = pd.DataFrame(chs, columns=['PAN', 'FL', 'CON', 'AMS', 'PMS', 'SLOT', 'NE'])\n",
        "    op_cols = ['AR', 'D1R', 'D2R', 'RR', 'D1L', 'TL', 'KS', 'MUL', 'DT1', 'DT2', 'AMS-EN']\n",
        "    m1s_df = pd.DataFrame(m1s, columns=op_cols)\n",
        "    c1s_df = pd.DataFrame(c1s, columns=op_cols)\n",
        "    m2s_df = pd.DataFrame(m2s, columns=op_cols)\n",
        "    c2s_df = pd.DataFrame(c2s, columns=op_cols)\n",
        "    \n",
        "    df = pd.concat([names_df, lfos_df, chs_df, \n",
        "                    m1s_df.add_prefix('M1_'), c1s_df.add_prefix('C1_'), \n",
        "                    m2s_df.add_prefix('M2_'), c2s_df.add_prefix('C2_')], axis=1)\n",
        "    df['Filename'] = filename\n",
        "    return df\n",
        "\n",
        "def process_opm_files(zip_filepath, output_filepath):\n",
        "    \"\"\"Process all OPM files from a zip archive.\"\"\"\n",
        "    all_dfs = []\n",
        "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "        opm_files = [f for f in zip_ref.namelist() if f.endswith('.opm')]\n",
        "        print(f\"üìÅ Found {len(opm_files)} OPM files\")\n",
        "        for i, file in enumerate(opm_files):\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"   Processing: {i + 1}/{len(opm_files)}\")\n",
        "            with zip_ref.open(file, 'r') as opm_file:\n",
        "                contents = opm_file.read().decode('utf-8')\n",
        "            all_dfs.append(parse_opm(contents, file))\n",
        "    \n",
        "    result = pd.concat(all_dfs, ignore_index=True)\n",
        "    result.to_csv(output_filepath, index=False)\n",
        "    print(f\"‚úÖ Saved: {output_filepath}\")\n",
        "    print(f\"   Total presets: {len(result):,}\")\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Paths\n",
        "zip_filepath = \"data/raw/OPM presets.zip\"\n",
        "output_filepath = \"data/processed/all_instruments_final.csv\"\n",
        "\n",
        "# Check if already exists\n",
        "if Path(output_filepath).exists():\n",
        "    print(\"‚ö†Ô∏è WARNING: all_instruments_final.csv already exists.\")\n",
        "    print(f\"   To re-process, delete: {output_filepath}\")\n",
        "    df_presets = pd.read_csv(output_filepath)\n",
        "    print(f\"   ‚úÖ Loaded {len(df_presets):,} presets from CSV\")\n",
        "else:\n",
        "    print(\"üîÑ Processing OPM files...\")\n",
        "    df_presets = process_opm_files(zip_filepath, output_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Game Name Extraction\n",
        "\n",
        "### 6.1 Extracting Game Names from Filenames\n",
        "\n",
        "The `Filename` column contains the source file name, which follows the pattern: `GameName_-_TrackName.opm`\n",
        "\n",
        "**The Challenge:**\n",
        "- Filenames use underscores instead of spaces\n",
        "- Some games have multiple variations\n",
        "- Track names are mixed in\n",
        "- Need to normalize for consistency\n",
        "\n",
        "### 6.2 Normalization: Handling Variations\n",
        "\n",
        "Game names can appear in many forms:\n",
        "- `Sonic_the_Hedgehog_-_Green_Hill_Zone.opm`\n",
        "- `Sonic The Hedgehog`\n",
        "- `Sonic the Hedgehog (USA)`\n",
        "- `Sonic_the_Hedgehog_2`\n",
        "\n",
        "**Our Normalization Strategy:**\n",
        "1. Extract game name from filename pattern\n",
        "2. Replace underscores with spaces\n",
        "3. Remove track names (after `_-_`)\n",
        "4. Clean up special characters\n",
        "5. Standardize capitalization\n",
        "\n",
        "### 6.3 The Challenge of Inconsistent Naming\n",
        "\n",
        "**Why This Matters:**\n",
        "- Same game may appear with different names\n",
        "- Makes grouping and analysis difficult\n",
        "- Affects composer matching\n",
        "- Impacts data quality\n",
        "\n",
        "**Solutions:**\n",
        "- Fuzzy matching for similar names\n",
        "- Manual mapping for known variations\n",
        "- Community verification (VGMrips helps)\n",
        "- Normalization rules\n",
        "\n",
        "**Example Variations:**\n",
        "- `Sonic the Hedgehog` vs `Sonic The Hedgehog`\n",
        "- `Streets of Rage` vs `Bare Knuckle` (Japanese name)\n",
        "- `Golden Axe` vs `Golden Axe (USA)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract game names from filenames\n",
        "def extract_game_name(filename):\n",
        "    \"\"\"Extract game name from filename pattern: GameName_-_TrackName.opm\"\"\"\n",
        "    if pd.isna(filename):\n",
        "        return \"Unknown\"\n",
        "    name = filename.replace('.opm', '')\n",
        "    parts = name.split('_-_')\n",
        "    if len(parts) > 0:\n",
        "        game = parts[0].replace('_', ' ')\n",
        "        return game\n",
        "    return \"Unknown\"\n",
        "\n",
        "df_presets['Game'] = df_presets['Filename'].apply(extract_game_name)\n",
        "\n",
        "print(f\"üéÆ Unique games: {df_presets['Game'].nunique():,}\")\n",
        "print(f\"\\nTop 10 games by preset count:\")\n",
        "print(df_presets['Game'].value_counts().head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Duplicate Detection and Removal\n",
        "\n",
        "### 7.1 Why Duplicates Exist: The Volume Problem\n",
        "\n",
        "When extracting presets from VGM files, we often find the **same preset** appearing multiple times. Why?\n",
        "\n",
        "**The Volume Problem:**\n",
        "- The same sound preset can be used at different volumes\n",
        "- Games adjust volume (TL parameter) for mixing\n",
        "- A \"Bass\" preset might appear 10 times with different TL values\n",
        "- But the **timbral character** (all other parameters) is identical\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Preset 1: Bass (TL=0)   - Full volume\n",
        "Preset 2: Bass (TL=10)  - Quieter\n",
        "Preset 3: Bass (TL=20)  - Even quieter\n",
        "```\n",
        "\n",
        "All three are the **same sound**, just at different volumes.\n",
        "\n",
        "### 7.2 Same Preset, Different Volumes (TL Parameter)\n",
        "\n",
        "**TL (Total Level)** controls volume, not timbre:\n",
        "\n",
        "- **TL = 0**: Maximum volume\n",
        "- **TL = 127**: Silent (muted)\n",
        "- **TL = 63**: Half volume\n",
        "\n",
        "**Why This Happens:**\n",
        "- **Mixing**: Games need different volumes for balance\n",
        "- **Dynamics**: Volume changes for musical expression\n",
        "- **Layering**: Multiple instances of the same sound at different volumes\n",
        "- **Efficiency**: Reuse presets instead of creating new ones\n",
        "\n",
        "**The Data Science Impact:**\n",
        "- Duplicates inflate dataset size\n",
        "- Don't add new information (same timbre)\n",
        "- Can bias analysis (same sound counted multiple times)\n",
        "- Need to remove for accurate analysis\n",
        "\n",
        "### 7.3 The Deduplication Strategy: Per-Game Analysis\n",
        "\n",
        "**Our Strategy:**\n",
        "1. **Group by Game**: Process each game separately\n",
        "2. **Compare Parameters**: All parameters except TL\n",
        "3. **Keep First**: When duplicates found, keep the first occurrence\n",
        "4. **Preserve Cross-Game**: Same preset in different games = valuable data\n",
        "\n",
        "**Why Per-Game?**\n",
        "- Same preset in different games = **different creative context**\n",
        "- Shows how composers reused sounds\n",
        "- Reveals patterns across games\n",
        "- Preserves valuable information\n",
        "\n",
        "**Why Remove Within-Game?**\n",
        "- Same preset, same game, different volume = **redundant**\n",
        "- Doesn't add new information\n",
        "- Inflates dataset size\n",
        "- Can bias analysis\n",
        "\n",
        "### 7.4 Why We Keep Cross-Game Duplicates\n",
        "\n",
        "**Cross-game duplicates are valuable:**\n",
        "\n",
        "- **Composer Signatures**: Shows preferred sounds\n",
        "- **Tool Usage**: Reveals GEMS vs custom tools\n",
        "- **Evolution**: How sounds changed over time\n",
        "- **Patterns**: Common presets across games\n",
        "\n",
        "**Example:**\n",
        "- \"Bass 1\" appears in 50 games\n",
        "- This tells us it was a popular/effective preset\n",
        "- Shows composer preferences\n",
        "- Reveals tool usage (GEMS templates)\n",
        "\n",
        "### 7.5 The Importance of Clean Data\n",
        "\n",
        "**Why Clean Data Matters:**\n",
        "\n",
        "1. **Accurate Analysis**: Clean data = reliable results\n",
        "2. **Efficient Processing**: Smaller dataset = faster ML\n",
        "3. **Pattern Discovery**: Real patterns, not duplicates\n",
        "4. **Reproducibility**: Consistent results across runs\n",
        "\n",
        "**Our Cleaning Process:**\n",
        "- Remove TL-based duplicates within games\n",
        "- Preserve cross-game duplicates\n",
        "- Validate parameter ranges\n",
        "- Check for missing data\n",
        "- Normalize game names\n",
        "\n",
        "**Result:**\n",
        "- **93,000+ presets** ‚Üí **~70,000 unique timbres** (estimated)\n",
        "- Same timbral information\n",
        "- Reduced redundancy\n",
        "- Ready for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remove duplicates within each game (same parameters except TL)\n",
        "print(\"üîç Removing duplicate presets within each game...\")\n",
        "print(f\"   Original dataset: {len(df_presets):,} presets\")\n",
        "\n",
        "# TL columns (volume) to exclude from comparison\n",
        "tl_cols = ['M1_TL', 'C1_TL', 'M2_TL', 'C2_TL']\n",
        "metadata_cols = ['Num', 'Name', 'Filename']\n",
        "\n",
        "# Columns to compare for duplicates (all except TL and metadata)\n",
        "cols_to_compare = [col for col in df_presets.columns \n",
        "                   if col not in tl_cols + metadata_cols + ['Game']]\n",
        "\n",
        "# Add Game to comparison so duplicates are removed WITHIN each game\n",
        "comparison_cols = ['Game'] + cols_to_compare\n",
        "\n",
        "# Remove duplicates\n",
        "df_dedup = df_presets.drop_duplicates(subset=comparison_cols, keep='first').reset_index(drop=True)\n",
        "\n",
        "removed_count = len(df_presets) - len(df_dedup)\n",
        "removed_pct = (removed_count / len(df_presets) * 100)\n",
        "\n",
        "print(f\"‚úÖ After removing duplicates: {len(df_dedup):,} presets\")\n",
        "print(f\"   Removed: {removed_count:,} duplicates ({removed_pct:.1f}%)\")\n",
        "print(f\"   Unique games: {df_dedup['Game'].nunique():,}\")\n",
        "\n",
        "# Update df_presets\n",
        "df_presets = df_dedup.copy()\n",
        "\n",
        "# Save cleaned dataset\n",
        "CLEANED_CSV = Path('artifacts/presets_cleaned.csv')\n",
        "df_presets.to_csv(CLEANED_CSV, index=False)\n",
        "print(f\"\\n‚úÖ Saved cleaned dataset to: {CLEANED_CSV}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART III: DATA ENRICHMENT - Adding Context\n",
        "\n",
        "## 8. GEMS Games: The American Sound Driver\n",
        "\n",
        "### 8.1 What Is GEMS?\n",
        "\n",
        "**GEMS** (Genesis Editor for Music and Sound) was Sega of America's official music tool for the Sega Genesis. Released in 1994, it was designed to make music creation accessible to American developers who didn't have the technical expertise to program the YM2612 chip directly.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![GEMS Interface](images/gems-interface.png)\n",
        "\n",
        "*GEMS - The tool that shaped American Genesis game music*  \n",
        "*Image: Sega Retro*\n",
        "\n",
        "</div>\n",
        "\n",
        "**What GEMS Provided:**\n",
        "- **Visual Interface**: No assembly language required\n",
        "- **Template Presets**: Pre-made sounds ready to use\n",
        "- **Music Editor**: Compose tracks visually\n",
        "- **Accessibility**: Made Genesis music creation easier\n",
        "\n",
        "**The Controversy:**\n",
        "- **Critics**: Called it \"generic\" and \"lazy\"\n",
        "- **Supporters**: Made music creation accessible\n",
        "- **Reality**: Used in 200+ games, shaped American Genesis sound\n",
        "\n",
        "### 8.2 The GEMS Controversy: Template Sounds vs. Custom\n",
        "\n",
        "**The Debate:**\n",
        "\n",
        "**Against GEMS:**\n",
        "- Same presets in multiple games = \"generic\" sound\n",
        "- Limited creativity compared to custom programming\n",
        "- Japanese games sounded more unique\n",
        "- Template-based approach = less artistic\n",
        "\n",
        "**For GEMS:**\n",
        "- Made music creation accessible to more developers\n",
        "- Faster development = more games with music\n",
        "- Consistent quality (templates were well-designed)\n",
        "- Enabled smaller studios to create music\n",
        "\n",
        "**The Reality:**\n",
        "- GEMS was a **tool**, not a limitation\n",
        "- Skilled composers could still create unique music\n",
        "- Many GEMS games have excellent soundtracks\n",
        "- The \"generic\" criticism is often overstated\n",
        "\n",
        "### 8.3 Sega Retro: The Registry of GEMS Games\n",
        "\n",
        "**Sega Retro** (segaretro.org) is a comprehensive wiki documenting all things Sega. It maintains a **complete list** of games that used GEMS, making it the authoritative source for GEMS game identification.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Sega Retro GEMS Page](images/segaretro-gems-page.png)\n",
        "\n",
        "*Sega Retro - The complete registry of GEMS games*  \n",
        "*Image: segaretro.org/GEMS*\n",
        "\n",
        "</div>\n",
        "\n",
        "**Why Sega Retro Is Valuable:**\n",
        "- **Complete List**: 200+ games documented\n",
        "- **Community Maintained**: Regularly updated\n",
        "- **Accurate**: Verified by the community\n",
        "- **Historical**: Preserves this information\n",
        "\n",
        "**The GEMS Page:**\n",
        "- Lists all known GEMS games\n",
        "- Includes release dates and regions\n",
        "- Documents GEMS versions\n",
        "- Provides historical context\n",
        "\n",
        "### 8.4 Extracting the GEMS Game List\n",
        "\n",
        "**The Challenge:**\n",
        "SegaRetro maintains a comprehensive list of games that used GEMS, but the website uses bot protection that blocks automated scraping. We use a **hybrid approach**:\n",
        "\n",
        "**1. Web Scraping (Primary Attempt):**\n",
        "- Try MediaWiki API (`action=parse`) - most reliable method\n",
        "- Fallback to `action=render` endpoint - lighter HTML\n",
        "- Last resort: direct page access\n",
        "- These methods may be blocked by bot protection\n",
        "\n",
        "**2. Local HTML File (Fallback):**\n",
        "- If web scraping fails or returns no results, load from `data/raw/GEMS.html`\n",
        "- This file should be manually saved from the browser\n",
        "- Provides reliable access when automated methods fail\n",
        "\n",
        "**How to Update the Data:**\n",
        "If you want the most up-to-date GEMS game list:\n",
        "1. Open https://segaretro.org/GEMS in your browser\n",
        "2. Save the page as HTML (Ctrl+S or File > Save As)\n",
        "3. Place the saved file in `data/raw/GEMS.html`\n",
        "4. Re-run the extraction cell\n",
        "\n",
        "**Our Parsing Strategy:**\n",
        "- **HTML Structure**: Find game lists in `div.mobile-columns`\n",
        "- **Extract Game Names**: Get text from links in list items\n",
        "- **Clean Names**: Remove years, tags, and formatting\n",
        "- **Save to CSV**: Cache results in `artifacts/gems_games.csv`\n",
        "\n",
        "### 8.5 Why GEMS Matters for Analysis\n",
        "\n",
        "**Data Science Perspective:**\n",
        "\n",
        "GEMS games represent a **controlled variable** in our dataset:\n",
        "\n",
        "- **Tool Usage**: All GEMS games used the same tool\n",
        "- **Template Presets**: Many shared presets\n",
        "- **Regional Pattern**: Mostly American games\n",
        "- **Time Period**: Concentrated in mid-1990s\n",
        "\n",
        "**What We Can Discover:**\n",
        "- **Tool Impact**: How GEMS affected sound design\n",
        "- **Template Usage**: Which presets were most popular\n",
        "- **Regional Differences**: GEMS vs. custom tools\n",
        "- **Evolution**: How GEMS usage changed over time\n",
        "\n",
        "**Source**: [segaretro.org/GEMS](https://segaretro.org/GEMS)  \n",
        "**Output**: `artifacts/gems_games.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def clean_game_name(game_text: str | None):\n",
        "    \"\"\"Clean game name by removing years, unreleased tags, etc.\"\"\"\n",
        "    if not game_text:\n",
        "        return None\n",
        "\n",
        "    game_text = game_text.strip()\n",
        "    if len(game_text) < 3:\n",
        "        return None\n",
        "\n",
        "    # Remove year in parentheses: \"Game Name (1994)\" -> \"Game Name\"\n",
        "    game_text = re.sub(r'\\s*\\(\\d{4}\\)\\s*', ' ', game_text)\n",
        "\n",
        "    # Remove \"(unreleased)\" or \"(released)\" tags\n",
        "    game_text = re.sub(r'\\s*\\((unreleased|released)\\)\\s*', '', game_text, flags=re.I)\n",
        "\n",
        "    # Remove other common tags in parentheses\n",
        "    game_text = re.sub(r'\\s*\\([^)]*test\\s*drive[^)]*\\)\\s*', '', game_text, flags=re.I)\n",
        "    game_text = re.sub(r'\\s*\\([^)]*page\\s*\\d+[^)]*\\)\\s*', '', game_text, flags=re.I)\n",
        "\n",
        "    # Clean up extra whitespace\n",
        "    game_text = ' '.join(game_text.split())\n",
        "\n",
        "    if len(game_text) < 3:\n",
        "        return None\n",
        "\n",
        "    # Skip if it looks like a section header or navigation\n",
        "    skip_patterns = [\n",
        "        'mega drive', 'sega cd', '32x', 'games which use', 'contents', 'history',\n",
        "        'usage', 'source code', 'external links', 'references'\n",
        "    ]\n",
        "    if any(pattern in game_text.lower() for pattern in skip_patterns):\n",
        "        return None\n",
        "\n",
        "    return game_text\n",
        "\n",
        "\n",
        "def _parse_games_from_html(html: str) -> list[str]:\n",
        "    \"\"\"Extract game names from an HTML document that contains SegaRetro content.\"\"\"\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    games = []\n",
        "\n",
        "    # Main structure: div.mobile-columns -> ul -> li -> a\n",
        "    for div in soup.select(\"div.mobile-columns\"):\n",
        "        for a in div.select(\"li a\"):\n",
        "            name = clean_game_name(a.get_text(strip=True))\n",
        "            if name:\n",
        "                games.append(name)\n",
        "\n",
        "    # Fallback: try main content area\n",
        "    if not games:\n",
        "        content = soup.find('div', {'id': 'mw-content-text'}) or soup\n",
        "        for li in content.select(\"ul li\"):\n",
        "            a = li.find('a')\n",
        "            if a:\n",
        "                name = clean_game_name(a.get_text(strip=True))\n",
        "                if name:\n",
        "                    games.append(name)\n",
        "\n",
        "    return sorted(set(games))\n",
        "\n",
        "\n",
        "def scrape_gems_games():\n",
        "    \"\"\"\n",
        "    Extract GEMS games list from SegaRetro.\n",
        "    \n",
        "    Strategy:\n",
        "    1. Try web scraping (MediaWiki API, render endpoint, or direct page)\n",
        "    2. If web scraping fails or returns no results, use local HTML file\n",
        "    3. If local file doesn't exist, return empty list with instructions\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    }\n",
        "    \n",
        "    # 1) Try MediaWiki API: action=parse\n",
        "    api_url = \"https://segaretro.org/api.php\"\n",
        "    api_params = {\n",
        "        \"action\": \"parse\",\n",
        "        \"page\": \"GEMS\",\n",
        "        \"prop\": \"text\",\n",
        "        \"format\": \"json\",\n",
        "        \"formatversion\": \"2\",\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(api_url, params=api_params, headers=headers, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        if r.text.strip().startswith('{'):\n",
        "            data = r.json()\n",
        "            if 'parse' in data and 'text' in data['parse']:\n",
        "                html = data[\"parse\"][\"text\"]\n",
        "                if isinstance(html, dict) and '*' in html:\n",
        "                    html = html['*']\n",
        "                games = _parse_games_from_html(html)\n",
        "                if games:\n",
        "                    print(f\"   ‚úÖ API: Found {len(games)} games\")\n",
        "                    return games\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 2) Try action=render\n",
        "    render_url = \"https://segaretro.org/index.php\"\n",
        "    render_params = {\"title\": \"GEMS\", \"action\": \"render\"}\n",
        "    try:\n",
        "        r = requests.get(render_url, params=render_params, headers=headers, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        games = _parse_games_from_html(r.text)\n",
        "        if games:\n",
        "            print(f\"   ‚úÖ Render: Found {len(games)} games\")\n",
        "            return games\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 3) Try normal page\n",
        "    url = \"https://segaretro.org/GEMS\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        games = _parse_games_from_html(r.text)\n",
        "        if games:\n",
        "            print(f\"   ‚úÖ Web page: Found {len(games)} games\")\n",
        "            return games\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 4) Fallback: Use local HTML file\n",
        "    local_path = Path(\"data/raw/GEMS.html\")\n",
        "    if local_path.exists():\n",
        "        try:\n",
        "            html = local_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "            games = _parse_games_from_html(html)\n",
        "            if games:\n",
        "                print(f\"   ‚úÖ Local file: Found {len(games)} games\")\n",
        "                return games\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Failed reading local file: {e}\")\n",
        "    \n",
        "    # 5) No data found\n",
        "    print(\"   ‚ö†Ô∏è No games found. Web scraping may be blocked by bot protection.\")\n",
        "    print(\"   üí° To get updated data, manually download the page:\")\n",
        "    print(\"      1. Open https://segaretro.org/GEMS in your browser\")\n",
        "    print(\"      2. Save the page as HTML (Ctrl+S or File > Save As)\")\n",
        "    print(\"      3. Place it in data/raw/GEMS.html\")\n",
        "    return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check cache and execute\n",
        "GEMS_CSV = Path('artifacts/gems_games.csv')\n",
        "GEMS_CSV.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if GEMS_CSV.exists():\n",
        "    print(\"‚ö†Ô∏è WARNING: gems_games.csv already exists. Loading from cache.\")\n",
        "    print(f\"   To re-scrape, delete: {GEMS_CSV}\")\n",
        "    gems_df = pd.read_csv(GEMS_CSV)\n",
        "    gems_games_list = gems_df['Game'].dropna().astype(str).tolist()\n",
        "    print(f\"   ‚úÖ Loaded {len(gems_games_list)} GEMS games\")\n",
        "else:\n",
        "    print(\"üîÑ Scraping GEMS games from SegaRetro...\")\n",
        "    gems_games_list = scrape_gems_games()\n",
        "    gems_df = pd.DataFrame({'Game': gems_games_list})\n",
        "    gems_df.to_csv(GEMS_CSV, index=False)\n",
        "    print(f\"   ‚úÖ Saved {len(gems_games_list)} games to {GEMS_CSV}\")\n",
        "\n",
        "# Optional: show a sample\n",
        "print(\"   Sample:\", gems_games_list[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 9. Composer Information: The Creative Context\n",
        "\n",
        "### 9.1 VGMrips: The Game-Composer Database\n",
        "\n",
        "**VGMrips** is our primary source for composer information. It maintains detailed records of who composed music for each game, making it invaluable for understanding the creative context behind the sounds.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![VGMrips YM2612 Packs](images/vgmrips-ym2612-packs.png)\n",
        "\n",
        "*VGMrips YM2612 packs - Organized by chip type*  \n",
        "*Image: vgmrips.net*\n",
        "\n",
        "</div>\n",
        "\n",
        "**What VGMrips Provides:**\n",
        "- **Game-Composer Mappings**: Who composed what\n",
        "- **Organized Packs**: Games grouped by chip (YM2612)\n",
        "- **Metadata**: Release dates, regions, additional info\n",
        "- **Community Verified**: User-contributed and verified data\n",
        "\n",
        "**Why VGMrips Is Reliable:**\n",
        "- **Community Maintained**: Many contributors\n",
        "- **Detailed Records**: Comprehensive information\n",
        "- **Regular Updates**: Continuously updated\n",
        "- **Historical Accuracy**: Preserves original information\n",
        "\n",
        "### 9.2 Scraping Game-Composer Mappings\n",
        "\n",
        "**The Challenge:**\n",
        "- VGMrips uses pagination (multiple pages)\n",
        "- HTML structure may vary\n",
        "- Need to handle edge cases\n",
        "- Respect rate limits\n",
        "\n",
        "**Our Approach:**\n",
        "1. **Iterate Pages**: Loop through all pages\n",
        "2. **Find Pack Items**: Locate game entries\n",
        "3. **Extract Game Names**: Get game titles\n",
        "4. **Extract Composers**: Find composer information\n",
        "5. **Handle Multiple Composers**: Split comma-separated lists\n",
        "6. **Save Progress**: Cache results to avoid re-scraping\n",
        "\n",
        "**Why This Matters:**\n",
        "- **Creative Context**: Know who created the sounds\n",
        "- **Style Analysis**: Compare composer techniques\n",
        "- **Regional Patterns**: Japanese vs. American composers\n",
        "- **Historical Record**: Preserve composer credits\n",
        "\n",
        "### 9.3 Wikipedia: The Composer Encyclopedia\n",
        "\n",
        "**Wikipedia** is our secondary source for composer information. While VGMrips provides game-composer mappings, Wikipedia provides **detailed biographical information**.\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "![Wikipedia Composer Page](images/wikipedia-composer-page.png)\n",
        "\n",
        "*Wikipedia - Rich biographical information about composers*  \n",
        "*Image: Wikipedia*\n",
        "\n",
        "</div>\n",
        "\n",
        "**What Wikipedia Provides:**\n",
        "- **Nationality**: Where composers are from\n",
        "- **Biography**: Career history and background\n",
        "- **Discography**: Other games they worked on\n",
        "- **Musical Style**: Their approach to composition\n",
        "\n",
        "**Why Wikipedia Is Valuable:**\n",
        "- **Comprehensive**: Covers many composers\n",
        "- **Detailed**: Rich biographical information\n",
        "- **Reliable**: Community-verified content\n",
        "- **Accessible**: Easy to search and extract\n",
        "\n",
        "### 9.4 Extracting Composer Information (Nationality, Biography)\n",
        "\n",
        "**The Search Process:**\n",
        "\n",
        "1. **Search for Game**: \"Sonic the Hedgehog Sega Genesis soundtrack\"\n",
        "2. **Find Composer**: Extract composer name from article\n",
        "3. **Search Composer**: Look up composer's Wikipedia page\n",
        "4. **Extract Nationality**: Find nationality information\n",
        "5. **Extract Biography**: Get career information\n",
        "\n",
        "**Challenges:**\n",
        "- **Multiple Composers**: Games may have multiple composers\n",
        "- **Name Variations**: Same composer, different names\n",
        "- **Missing Information**: Not all composers have Wikipedia pages\n",
        "- **Language Barriers**: Some information in Japanese\n",
        "\n",
        "**Our Approach:**\n",
        "- **Fuzzy Matching**: Handle name variations\n",
        "- **Multiple Searches**: Try different search terms\n",
        "- **Fallback Data**: Use VGMrips as primary source\n",
        "- **Manual Mapping**: Known composers mapped manually\n",
        "\n",
        "### 9.5 The Challenge of Incomplete Data\n",
        "\n",
        "**The Reality:**\n",
        "- Not all games have composer information\n",
        "- Not all composers have Wikipedia pages\n",
        "- Some information is incomplete or inaccurate\n",
        "- Historical records may be lost\n",
        "\n",
        "**How We Handle It:**\n",
        "- **Primary Source**: VGMrips (most reliable)\n",
        "- **Secondary Source**: Wikipedia (for enrichment)\n",
        "- **Manual Mapping**: Known composers added manually\n",
        "- **Accept Incompleteness**: Some games will have \"Unknown\" composer\n",
        "\n",
        "**Why This Is OK:**\n",
        "- **Partial Data**: Better than no data\n",
        "- **Enough for Analysis**: Most games have composer info\n",
        "- **Can Improve**: Can add more data later\n",
        "- **Realistic**: Reflects reality of historical data\n",
        "\n",
        "### 9.6 Why Composer Data Enriches Analysis\n",
        "\n",
        "**Creative Context:**\n",
        "\n",
        "Composer information transforms raw data into **stories**:\n",
        "\n",
        "- **Yuzo Koshiro**: Electronic music pioneer, Streets of Rage\n",
        "- **Masato Nakamura**: J-Pop star, Sonic the Hedgehog\n",
        "- **Hiroshi Kawaguchi**: Sega veteran, Golden Axe\n",
        "- **Howard Drossin**: American composer, Comix Zone\n",
        "\n",
        "**What We Can Discover:**\n",
        "- **Composer Signatures**: Unique parameter choices\n",
        "- **Regional Styles**: Japanese vs. American approaches\n",
        "- **Evolution**: How styles changed over time\n",
        "- **Collaborations**: Multiple composers on one game\n",
        "\n",
        "**Source**: [vgmrips.net/packs/chip/ym2612](https://vgmrips.net/packs/chip/ym2612)  \n",
        "**Output**: `artifacts/vgmrips_composers.csv`\n",
        "\n",
        "‚è±Ô∏è **Estimated time**: ~10-15 minutes (multiple pages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def scrape_vgmrips_ym2612_packs(max_pages=None):\n",
        "    \"\"\"\n",
        "    Scrape game-composer mappings from vgmrips.net/packs/chip/ym2612\n",
        "    Iterates through all pages to get complete mapping.\n",
        "    \n",
        "    Returns: list of dicts with 'Game', 'Composers' (list), and 'Source'\n",
        "    \"\"\"\n",
        "    game_composer_mappings = []\n",
        "    base_url = \"https://vgmrips.net/packs/chip/ym2612\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    \n",
        "    page = 0\n",
        "    total_packs = 0\n",
        "    \n",
        "    print(\"üéµ Scraping YM2612 packs from VGMrips...\")\n",
        "    print(f\"   Starting from: {base_url}\")\n",
        "    \n",
        "    while True:\n",
        "        # Build URL for current page\n",
        "        if page == 0:\n",
        "            url = base_url\n",
        "        else:\n",
        "            url = f\"{base_url}?p={page}\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # Find all pack entries - based on the HTML structure, packs are in sections\n",
        "            # Look for h2 tags with game titles (these are the pack titles)\n",
        "            pack_titles = soup.find_all('h2')\n",
        "            \n",
        "            page_packs = 0\n",
        "            for title_elem in pack_titles:\n",
        "                try:\n",
        "                    # Get the game title from h2\n",
        "                    game_title = title_elem.get_text(strip=True)\n",
        "                    \n",
        "                    # Clean game title - remove extra info like \"(W) / (J)\" etc.\n",
        "                    # Example: \"Thunder Force IV (W) / Lightening Force: Quest for the Darkstar (U)\"\n",
        "                    game_title = re.sub(r'\\s*\\([^)]+\\)\\s*', ' ', game_title)  # Remove (W), (J), etc.\n",
        "                    game_title = re.sub(r'\\s*/\\s*.*$', '', game_title)  # Remove \" / Alternative Name\"\n",
        "                    game_title = game_title.strip()\n",
        "                    \n",
        "                    if not game_title or len(game_title) < 3:\n",
        "                        continue\n",
        "                    \n",
        "                    # Find the parent container (usually a div or article)\n",
        "                    pack_container = title_elem.find_parent(['div', 'article', 'section'])\n",
        "                    if not pack_container:\n",
        "                        pack_container = title_elem.find_next_sibling()\n",
        "                    \n",
        "                    # Find composers - look for table with \"Composer:\" row\n",
        "                    composers = []\n",
        "                    if pack_container:\n",
        "                        # Look for all links to composer pages\n",
        "                        composer_links = pack_container.find_all('a', href=re.compile(r'/packs/composer/'))\n",
        "                        for link in composer_links:\n",
        "                            composer_name = link.get_text(strip=True)\n",
        "                            if composer_name and composer_name not in composers:\n",
        "                                composers.append(composer_name)\n",
        "                    \n",
        "                    # Alternative: search in the entire page section after the title\n",
        "                    if not composers:\n",
        "                        # Find the table or section with composer info\n",
        "                        next_elements = title_elem.find_all_next(['table', 'div', 'p'], limit=5)\n",
        "                        for elem in next_elements:\n",
        "                            composer_links = elem.find_all('a', href=re.compile(r'/packs/composer/'))\n",
        "                            for link in composer_links:\n",
        "                                composer_name = link.get_text(strip=True)\n",
        "                                if composer_name and composer_name not in composers:\n",
        "                                    composers.append(composer_name)\n",
        "                            if composers:\n",
        "                                break\n",
        "                    \n",
        "                    # If we found a game title and at least one composer, add it\n",
        "                    if game_title and composers:\n",
        "                        game_composer_mappings.append({\n",
        "                            'Game': game_title,\n",
        "                            'Composers': composers,\n",
        "                            'Source': 'VGMrips'\n",
        "                        })\n",
        "                        page_packs += 1\n",
        "                        total_packs += 1\n",
        "                \n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            # Check if there are more pages\n",
        "            next_page_link = soup.find('a', string=re.compile(r'next|¬ª', re.I))\n",
        "            has_next_page = next_page_link is not None\n",
        "            \n",
        "            # Also check pagination info (e.g., \"Packs 1 to 20 of 795 total\")\n",
        "            pagination_text = soup.find(string=re.compile(r'Packs.*of.*total', re.I))\n",
        "            if pagination_text:\n",
        "                total_match = re.search(r'of\\s+(\\d+)\\s+total', pagination_text)\n",
        "                if total_match:\n",
        "                    total_expected = int(total_match.group(1))\n",
        "                    if total_packs >= total_expected:\n",
        "                        has_next_page = False\n",
        "            \n",
        "            print(f\"   Page {page}: Found {page_packs} packs (Total: {total_packs})\")\n",
        "            \n",
        "            # Stop if no more pages or max_pages reached\n",
        "            if not has_next_page:\n",
        "                print(f\"   ‚úÖ Reached end. Total packs scraped: {total_packs}\")\n",
        "                break\n",
        "            \n",
        "            if max_pages and page >= max_pages:\n",
        "                print(f\"   ‚ö†Ô∏è Reached max_pages limit ({max_pages})\")\n",
        "                break\n",
        "            \n",
        "            page += 1\n",
        "            time.sleep(1)  # Be respectful to the server\n",
        "        \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"   ‚ö†Ô∏è Error fetching page {page}: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Error parsing page {page}: {e}\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\n‚úÖ Scraping complete: {len(game_composer_mappings)} game-composer mappings found\")\n",
        "    return game_composer_mappings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check cache and execute\n",
        "VGMRIPS_CSV = Path('artifacts/vgmrips_composers.csv')\n",
        "VGMRIPS_CSV.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if VGMRIPS_CSV.exists():\n",
        "    print(\"‚ö†Ô∏è WARNING: vgmrips_composers.csv already exists. Loading from cache.\")\n",
        "    print(f\"   To re-scrape, delete: {VGMRIPS_CSV}\")\n",
        "    vgmrips_composers_df = pd.read_csv(VGMRIPS_CSV)\n",
        "    print(f\"   ‚úÖ Loaded {len(vgmrips_composers_df)} mappings\")\n",
        "    print(f\"   Unique games: {vgmrips_composers_df['Game'].nunique()}\")\n",
        "    print(f\"   Unique composers: {vgmrips_composers_df['Composer'].nunique()}\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéµ Loading game-composer mappings from VGMrips YM2612 packs...\")\n",
        "    print(\"   Source: https://vgmrips.net/packs/chip/ym2612\")\n",
        "    print(\"   This may take a few minutes...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # For testing, you can limit pages with max_pages parameter\n",
        "    # Remove max_pages to scrape all pages\n",
        "    vgmrips_mappings = scrape_vgmrips_ym2612_packs(max_pages=None)  # Set to None for all pages\n",
        "\n",
        "    # Convert to DataFrame for easier manipulation\n",
        "    if vgmrips_mappings:\n",
        "        vgmrips_df = pd.DataFrame(vgmrips_mappings)\n",
        "        # Expand composers list into separate rows (one per composer)\n",
        "        vgmrips_expanded = []\n",
        "        for _, row in vgmrips_df.iterrows():\n",
        "            for composer in row['Composers']:\n",
        "                vgmrips_expanded.append({\n",
        "                    'Game': row['Game'],\n",
        "                    'Composer': composer,\n",
        "                    'Source': row['Source']\n",
        "                })\n",
        "        vgmrips_composers_df = pd.DataFrame(vgmrips_expanded)\n",
        "        \n",
        "        vgmrips_composers_df.to_csv(VGMRIPS_CSV, index=False)\n",
        "        \n",
        "        print(f\"\\nüìä Summary:\")\n",
        "        print(f\"   Unique games: {vgmrips_composers_df['Game'].nunique()}\")\n",
        "        print(f\"   Unique composers: {vgmrips_composers_df['Composer'].nunique()}\")\n",
        "        print(f\"   Total mappings: {len(vgmrips_composers_df)}\")\n",
        "        \n",
        "        print(f\"\\n   Top 10 games by composer count:\")\n",
        "        game_composer_counts = vgmrips_composers_df.groupby('Game')['Composer'].count().sort_values(ascending=False).head(10)\n",
        "        for game, count in game_composer_counts.items():\n",
        "            composers = vgmrips_composers_df[vgmrips_composers_df['Game'] == game]['Composer'].unique()\n",
        "            print(f\"      {game}: {count} composer(s) - {', '.join(composers[:3])}{'...' if len(composers) > 3 else ''}\")\n",
        "        \n",
        "        print(f\"\\n   Top 10 composers by game count:\")\n",
        "        composer_game_counts = vgmrips_composers_df.groupby('Composer')['Game'].count().sort_values(ascending=False).head(10)\n",
        "        for composer, count in composer_game_counts.items():\n",
        "            print(f\"      {composer}: {count} games\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Saved {len(vgmrips_composers_df)} mappings to {VGMRIPS_CSV}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No mappings found. Check the scraper implementation.\")\n",
        "        vgmrips_composers_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.7 Wikipedia Scraping Implementation\n",
        "\n",
        "Searches for additional composer information (nationality, biography) from Wikipedia.\n",
        "\n",
        "**Source**: Wikipedia  \n",
        "**Output**: `artifacts/composers_info.csv`\n",
        "\n",
        "‚è±Ô∏è **Estimated time**: Variable depending on number of unique games"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def search_composer_info(game_name):\n",
        "    \"\"\"\n",
        "    Search for composer information from VGMdb, Wikipedia, MobyGames, and other sources.\n",
        "    Returns: dict with composer name and nationality, or None if not found\n",
        "    \"\"\"\n",
        "    composer_name = None\n",
        "    nationality = None\n",
        "    source = None\n",
        "    \n",
        "    # Clean game name for searching\n",
        "    search_terms = [\n",
        "        f\"{game_name} Sega Genesis soundtrack\",\n",
        "        f\"{game_name} Mega Drive composer\",\n",
        "        f\"{game_name} music composer\"\n",
        "    ]\n",
        "    \n",
        "    # Try Wikipedia\n",
        "    try:\n",
        "        for term in search_terms:\n",
        "            try:\n",
        "                search_results = wikipedia.search(term, results=5)\n",
        "                for result in search_results:\n",
        "                    try:\n",
        "                        page = wikipedia.page(result, auto_suggest=False)\n",
        "                        content = (page.summary + \" \" + page.content[:2000]).lower()\n",
        "                        \n",
        "                        # Look for composer mentions\n",
        "                        if 'composer' in content or 'music' in content or 'soundtrack' in content:\n",
        "                            # Try to extract composer name (simplified pattern matching)\n",
        "                            # Common patterns: \"composed by X\", \"music by X\", \"X composed\"\n",
        "                            patterns = [\n",
        "                                r'composed by ([A-Z][a-z]+ [A-Z][a-z]+)',\n",
        "                                r'music by ([A-Z][a-z]+ [A-Z][a-z]+)',\n",
        "                                r'soundtrack by ([A-Z][a-z]+ [A-Z][a-z]+)',\n",
        "                                r'([A-Z][a-z]+ [A-Z][a-z]+) composed',\n",
        "                            ]\n",
        "                            \n",
        "                            for pattern in patterns:\n",
        "                                matches = re.findall(pattern, page.content[:3000])\n",
        "                                if matches:\n",
        "                                    composer_name = matches[0]\n",
        "                                    source = 'Wikipedia'\n",
        "                                    break\n",
        "                            \n",
        "                            if composer_name:\n",
        "                                # Try to determine nationality from Wikipedia page\n",
        "                                if composer_name:\n",
        "                                    try:\n",
        "                                        composer_page = wikipedia.page(composer_name, auto_suggest=True)\n",
        "                                        composer_content = composer_page.summary.lower()\n",
        "                                        if 'japanese' in composer_content or 'japan' in composer_content:\n",
        "                                            nationality = 'Japan'\n",
        "                                        elif 'american' in composer_content or 'united states' in composer_content:\n",
        "                                            nationality = 'USA'\n",
        "                                        elif 'british' in composer_content or 'uk' in composer_content:\n",
        "                                            nationality = 'UK'\n",
        "                                        elif 'european' in composer_content:\n",
        "                                            nationality = 'Europe'\n",
        "                                    except:\n",
        "                                        pass\n",
        "                                break\n",
        "                    except:\n",
        "                        continue\n",
        "                if composer_name:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    \n",
        "    # Try VGMdb (via web search - in production you'd use their API)\n",
        "    if not composer_name:\n",
        "        try:\n",
        "            # Search VGMdb via web\n",
        "            vgmdb_url = f\"https://vgmdb.net/search?q={game_name.replace(' ', '+')}\"\n",
        "            # Note: In production, use VGMdb API if available\n",
        "            # For now, we'll rely on Wikipedia results\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Try MobyGames (via web search)\n",
        "    if not composer_name:\n",
        "        try:\n",
        "            mobygames_url = f\"https://www.mobygames.com/search/quick?q={game_name.replace(' ', '+')}\"\n",
        "            # Note: In production, use MobyGames API if available\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Add delay to be respectful to servers\n",
        "    time.sleep(1)\n",
        "    \n",
        "    if composer_name:\n",
        "        return {\n",
        "            'Composer': composer_name,\n",
        "            'Nationality': nationality if nationality else 'Unknown',\n",
        "            'Source': source\n",
        "        }\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check cache and execute\n",
        "COMPOSER_CSV = Path('artifacts/composers_info.csv')\n",
        "COMPOSER_CSV.parent.mkdir(exist_ok=True)\n",
        "\n",
        "# Get unique games from presets\n",
        "# Use the same extract_game_name function that was used earlier in the notebook\n",
        "if 'df_presets' in dir():\n",
        "    # Check if Game column already exists (extracted earlier)\n",
        "    if 'Game' not in df_presets.columns:\n",
        "        df_presets['Game'] = df_presets['Filename'].apply(extract_game_name)\n",
        "    unique_games = df_presets['Game'].unique().tolist()\n",
        "else:\n",
        "    df_presets = pd.read_csv('data/processed/all_instruments_final.csv')\n",
        "    df_presets['Game'] = df_presets['Filename'].apply(extract_game_name)\n",
        "    unique_games = df_presets['Game'].unique().tolist()\n",
        "\n",
        "num_games = len(unique_games)\n",
        "\n",
        "if COMPOSER_CSV.exists():\n",
        "    print(\"‚ö†Ô∏è WARNING: composers_info.csv already exists. Loading from cache.\")\n",
        "    print(f\"   To re-scrape, delete: {COMPOSER_CSV}\")\n",
        "    composer_df = pd.read_csv(COMPOSER_CSV)\n",
        "    print(f\"   ‚úÖ Loaded {len(composer_df)} composers\")\n",
        "    if 'Nationality' in composer_df.columns:\n",
        "        print(f\"\\nüåç Nationality distribution:\")\n",
        "        print(composer_df['Nationality'].value_counts())\n",
        "else:\n",
        "    print(f\"üéµ Searching composer information for {num_games:,} unique games...\")\n",
        "    print(f\"   This will query Wikipedia, VGMdb, and MobyGames for each game\")\n",
        "    print(f\"   Estimated time: ~{num_games * 1.5 / 60:.1f} minutes\")\n",
        "    print(f\"   ‚ö†Ô∏è This may take a while. Results will be cached to avoid re-querying.\")\n",
        "\n",
        "    # Search for composers (with progress indication)\n",
        "    composer_data = []\n",
        "    found_count = 0\n",
        "    not_found_games = []\n",
        "\n",
        "    for i, game in enumerate(unique_games, 1):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"   Progress: {i}/{num_games} games searched ({found_count} composers found)\")\n",
        "        \n",
        "        result = search_composer_info(game)\n",
        "        if result:\n",
        "            result['Game'] = game\n",
        "            composer_data.append(result)\n",
        "            found_count += 1\n",
        "        else:\n",
        "            not_found_games.append(game)\n",
        "\n",
        "    print(f\"\\n‚úÖ Found composer information for {found_count}/{num_games} games ({found_count/num_games*100:.1f}%)\")\n",
        "\n",
        "    # Create composer dataframe\n",
        "    if composer_data:\n",
        "        composer_df = pd.DataFrame(composer_data)\n",
        "        composer_df.to_csv(COMPOSER_CSV, index=False)\n",
        "        \n",
        "        print(f\"\\nüìä Composer information summary:\")\n",
        "        print(composer_df[['Game', 'Composer', 'Nationality', 'Source']].head(20))\n",
        "        \n",
        "        # Show nationality distribution\n",
        "        if 'Nationality' in composer_df.columns:\n",
        "            print(f\"\\nüåç Nationality distribution:\")\n",
        "            print(composer_df['Nationality'].value_counts())\n",
        "        \n",
        "        print(f\"\\n‚úÖ Saved {len(composer_df)} composers to {COMPOSER_CSV}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No composer information found. Using fallback manual mapping.\")\n",
        "        # Fallback to manual mapping for well-known games\n",
        "        composer_mapping = {\n",
        "            'Sonic the Hedgehog': ['Masato Nakamura', 'Japan'],\n",
        "            'Streets of Rage': ['Yuzo Koshiro', 'Japan'],\n",
        "            'Streets of Rage 2': ['Yuzo Koshiro', 'Japan'],\n",
        "            'Shinobi III': ['Yuzo Koshiro', 'Japan'],\n",
        "            'Golden Axe': ['Hiroshi Kawaguchi', 'Japan'],\n",
        "            'Phantasy Star IV': ['Izuho Takeuchi', 'Japan'],\n",
        "            'Gunstar Heroes': ['Norio Hanzawa', 'Japan'],\n",
        "            'Comix Zone': ['Howard Drossin', 'USA'],\n",
        "            'Ecco the Dolphin': ['Spencer Nilsen', 'USA'],\n",
        "            'ToeJam & Earl': ['John Baker', 'USA'],\n",
        "        }\n",
        "        \n",
        "        composer_data = []\n",
        "        for game, info in composer_mapping.items():\n",
        "            if game in unique_games:\n",
        "                composer_data.append({\n",
        "                    'Game': game,\n",
        "                    'Composer': info[0],\n",
        "                    'Nationality': info[1],\n",
        "                    'Source': 'Manual'\n",
        "                })\n",
        "        \n",
        "        composer_df = pd.DataFrame(composer_data) if composer_data else pd.DataFrame()\n",
        "        if len(composer_df) > 0:\n",
        "            composer_df.to_csv(COMPOSER_CSV, index=False)\n",
        "            print(f\"   ‚úÖ Saved {len(composer_df)} manual mappings to {COMPOSER_CSV}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# PART IV: DATA QUALITY AND PROCESSING\n",
        "\n",
        "## 10. Data Validation and Quality Checks\n",
        "\n",
        "Validate parameter ranges, check completeness, and ensure data consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validation functions\n",
        "def validate_parameter_ranges(df):\n",
        "    \"\"\"Validate that all parameters are within expected ranges.\"\"\"\n",
        "    errors = []\n",
        "    \n",
        "    # CON (Algorithm): 0-7\n",
        "    if 'CON' in df.columns:\n",
        "        invalid = df[(df['CON'] < 0) | (df['CON'] > 7)]\n",
        "        if len(invalid) > 0:\n",
        "            errors.append(f\"CON: {len(invalid)} invalid values\")\n",
        "    \n",
        "    # FL (Feedback): 0-7\n",
        "    if 'FL' in df.columns:\n",
        "        invalid = df[(df['FL'] < 0) | (df['FL'] > 7)]\n",
        "        if len(invalid) > 0:\n",
        "            errors.append(f\"FL: {len(invalid)} invalid values\")\n",
        "    \n",
        "    # AR (Attack Rate): 0-31\n",
        "    ar_cols = [c for c in df.columns if c.startswith('AR')]\n",
        "    for col in ar_cols:\n",
        "        invalid = df[(df[col] < 0) | (df[col] > 31)]\n",
        "        if len(invalid) > 0:\n",
        "            errors.append(f\"{col}: {len(invalid)} invalid values\")\n",
        "    \n",
        "    # TL (Total Level): 0-127\n",
        "    tl_cols = [c for c in df.columns if c.startswith('TL')]\n",
        "    for col in tl_cols:\n",
        "        invalid = df[(df[col] < 0) | (df[col] > 127)]\n",
        "        if len(invalid) > 0:\n",
        "            errors.append(f\"{col}: {len(invalid)} invalid values\")\n",
        "    \n",
        "    # MUL (Multiplier): 0-15\n",
        "    mul_cols = [c for c in df.columns if c.startswith('MUL')]\n",
        "    for col in mul_cols:\n",
        "        invalid = df[(df[col] < 0) | (df[col] > 15)]\n",
        "        if len(invalid) > 0:\n",
        "            errors.append(f\"{col}: {len(invalid)} invalid values\")\n",
        "    \n",
        "    return errors\n",
        "\n",
        "def check_completeness(df):\n",
        "    \"\"\"Check for missing values in critical columns.\"\"\"\n",
        "    critical_cols = ['Game', 'CON', 'FL']\n",
        "    missing = {}\n",
        "    for col in critical_cols:\n",
        "        if col in df.columns:\n",
        "            count = df[col].isna().sum()\n",
        "            if count > 0:\n",
        "                missing[col] = count\n",
        "    return missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Data Cleaning and Standardization\n",
        "\n",
        "Normalize game and composer names, handle special characters, and fix data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cleaning functions\n",
        "def normalize_game_name(name):\n",
        "    \"\"\"Normalize game name: strip extra spaces, handle special cases.\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "    name = str(name).strip()\n",
        "    # Remove common suffixes\n",
        "    name = re.sub(r'\\s*\\([^)]*\\)\\s*$', '', name)  # Remove trailing (Year)\n",
        "    name = re.sub(r'\\s*\\[[^\\]]*\\]\\s*$', '', name)  # Remove trailing [Tag]\n",
        "    name = ' '.join(name.split())  # Normalize whitespace\n",
        "    return name if len(name) > 0 else None\n",
        "\n",
        "def normalize_composer_name(name):\n",
        "    \"\"\"Normalize composer name: standardize format.\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "    name = str(name).strip()\n",
        "    # Remove common prefixes/suffixes\n",
        "    name = re.sub(r'^\\s*(Mr\\.|Ms\\.|Dr\\.)\\s+', '', name, flags=re.I)\n",
        "    name = ' '.join(name.split())  # Normalize whitespace\n",
        "    return name if len(name) > 0 else None\n",
        "\n",
        "def clamp_parameters_to_valid_ranges(df):\n",
        "    \"\"\"\n",
        "    Clamp all YM2612 parameters to their valid ranges.\n",
        "    This ensures no out-of-range values that could cause issues in analysis.\n",
        "    \n",
        "    YM2612 Parameter Ranges:\n",
        "    - CON (Algorithm): 0-7\n",
        "    - FL (Feedback): 0-7\n",
        "    - AR (Attack Rate): 0-31\n",
        "    - D1R (Decay 1 Rate): 0-31\n",
        "    - D2R (Decay 2 Rate): 0-31\n",
        "    - RR (Release Rate): 0-15\n",
        "    - D1L (Decay 1 Level): 0-15\n",
        "    - TL (Total Level): 0-127\n",
        "    - KS (Key Scale): 0-3\n",
        "    - MUL (Multiplier): 0-15\n",
        "    - DT1 (Detune 1): 0-7\n",
        "    - DT2 (Detune 2): 0-3\n",
        "    - AMS-EN (AMS Enable): 0-1\n",
        "    - LFO params (LFRQ, AMD, PMD, WF, NFRQ): 0-255 (safe range)\n",
        "    - PAN: 0-3, AMS: 0-3, PMS: 0-7, SLOT: 0-15, NE: 0-1\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Define parameter ranges: {column_pattern: (min, max)}\n",
        "    param_ranges = {\n",
        "        'CON': (0, 7),\n",
        "        'FL': (0, 7),\n",
        "        'PAN': (0, 3),\n",
        "        'AMS': (0, 3),\n",
        "        'PMS': (0, 7),\n",
        "        'SLOT': (0, 15),\n",
        "        'NE': (0, 1),\n",
        "        'LFRQ': (0, 255),\n",
        "        'AMD': (0, 255),\n",
        "        'PMD': (0, 255),\n",
        "        'WF': (0, 3),\n",
        "        'NFRQ': (0, 31),\n",
        "    }\n",
        "    \n",
        "    # Operator parameters (apply to M1_, C1_, M2_, C2_ prefixes)\n",
        "    operator_param_ranges = {\n",
        "        'AR': (0, 31),\n",
        "        'D1R': (0, 31),\n",
        "        'D2R': (0, 31),\n",
        "        'RR': (0, 15),\n",
        "        'D1L': (0, 15),\n",
        "        'TL': (0, 127),\n",
        "        'KS': (0, 3),\n",
        "        'MUL': (0, 15),\n",
        "        'DT1': (0, 7),\n",
        "        'DT2': (0, 3),\n",
        "        'AMS-EN': (0, 1),\n",
        "    }\n",
        "    \n",
        "    clamped_count = 0\n",
        "    \n",
        "    # Clamp channel parameters\n",
        "    for param, (min_val, max_val) in param_ranges.items():\n",
        "        if param in df.columns:\n",
        "            before = df[param].copy()\n",
        "            df[param] = df[param].clip(lower=min_val, upper=max_val)\n",
        "            changed = (before != df[param]).sum()\n",
        "            if changed > 0:\n",
        "                clamped_count += changed\n",
        "                print(f\"      Clamped {param}: {changed} values to range [{min_val}, {max_val}]\")\n",
        "    \n",
        "    # Clamp operator parameters (M1_, C1_, M2_, C2_)\n",
        "    for prefix in ['M1_', 'C1_', 'M2_', 'C2_']:\n",
        "        for param, (min_val, max_val) in operator_param_ranges.items():\n",
        "            col = f\"{prefix}{param}\"\n",
        "            if col in df.columns:\n",
        "                before = df[col].copy()\n",
        "                df[col] = df[col].clip(lower=min_val, upper=max_val)\n",
        "                changed = (before != df[col]).sum()\n",
        "                if changed > 0:\n",
        "                    clamped_count += changed\n",
        "                    print(f\"      Clamped {col}: {changed} values to range [{min_val}, {max_val}]\")\n",
        "    \n",
        "    return df, clamped_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Final Dataset Assembly\n",
        "\n",
        "Join all tables, validate, and export the final dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check cache\n",
        "FINAL_CSV = Path('artifacts/presets_final.csv')\n",
        "FINAL_CSV.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if FINAL_CSV.exists():\n",
        "    print(\"‚ö†Ô∏è WARNING: presets_final.csv already exists. Loading from cache.\")\n",
        "    print(f\"   To re-generate, delete: {FINAL_CSV}\")\n",
        "    df_final = pd.read_csv(FINAL_CSV)\n",
        "    print(f\"   ‚úÖ Loaded {len(df_final):,} presets from cache\")\n",
        "else:\n",
        "    print(\"üîÑ Assembling final dataset...\")\n",
        "    \n",
        "    # Load all data sources\n",
        "    print(\"   üìÇ Loading data sources...\")\n",
        "    df_presets = pd.read_csv('artifacts/presets_cleaned.csv')\n",
        "    print(f\"      ‚úÖ Presets: {len(df_presets):,} records\")\n",
        "    \n",
        "    # Load GEMS games\n",
        "    gems_df = pd.read_csv('artifacts/gems_games.csv') if Path('artifacts/gems_games.csv').exists() else pd.DataFrame()\n",
        "    gems_games_list = gems_df['Game'].dropna().astype(str).tolist() if len(gems_df) > 0 else []\n",
        "    print(f\"      ‚úÖ GEMS games: {len(gems_games_list)} games\")\n",
        "    \n",
        "    # Load composer info\n",
        "    composer_df = pd.read_csv('artifacts/composers_info.csv') if Path('artifacts/composers_info.csv').exists() else pd.DataFrame()\n",
        "    print(f\"      ‚úÖ Composer info: {len(composer_df):,} mappings\")\n",
        "    \n",
        "    # Start with presets\n",
        "    df_final = df_presets.copy()\n",
        "    \n",
        "    # Normalize game names\n",
        "    print(\"   üßπ Normalizing game names...\")\n",
        "    df_final['Game'] = df_final['Game'].apply(normalize_game_name)\n",
        "    \n",
        "    # Add GEMS flag\n",
        "    print(\"   üéÆ Adding GEMS information...\")\n",
        "    df_final['Uses_GEMS'] = df_final['Game'].apply(\n",
        "        lambda x: x in gems_games_list if pd.notna(x) else False\n",
        "    )\n",
        "    \n",
        "    # Merge composer information\n",
        "    if len(composer_df) > 0:\n",
        "        print(\"   üéµ Merging composer information...\")\n",
        "        # Normalize composer names\n",
        "        composer_df['Game'] = composer_df['Game'].apply(normalize_game_name)\n",
        "        composer_df['Composer'] = composer_df['Composer'].apply(normalize_composer_name)\n",
        "        \n",
        "        # Merge (keep first composer if multiple)\n",
        "        composer_df_unique = composer_df.groupby('Game').first().reset_index()\n",
        "        df_final = df_final.merge(\n",
        "            composer_df_unique[['Game', 'Composer', 'Nationality']],\n",
        "            on='Game',\n",
        "            how='left'\n",
        "        )\n",
        "    else:\n",
        "        df_final['Composer'] = None\n",
        "        df_final['Nationality'] = None\n",
        "    \n",
        "    # Validate data\n",
        "    print(\"   ‚úÖ Validating data...\")\n",
        "    range_errors = validate_parameter_ranges(df_final)\n",
        "    if range_errors:\n",
        "        print(f\"      ‚ö†Ô∏è Found {len(range_errors)} range validation issues\")\n",
        "        for error in range_errors[:5]:  # Show first 5\n",
        "            print(f\"         - {error}\")\n",
        "    else:\n",
        "        print(\"      ‚úÖ All parameters within valid ranges\")\n",
        "    \n",
        "    missing = check_completeness(df_final)\n",
        "    if missing:\n",
        "        print(f\"      ‚ö†Ô∏è Found missing values:\")\n",
        "        for col, count in missing.items():\n",
        "            print(f\"         - {col}: {count} missing\")\n",
        "    else:\n",
        "        print(\"      ‚úÖ No missing values in critical columns\")\n",
        "    \n",
        "    # Clamp parameters to valid ranges (fix out-of-range values)\n",
        "    print(\"   üìê Clamping parameters to valid ranges...\")\n",
        "    df_final, total_clamped = clamp_parameters_to_valid_ranges(df_final)\n",
        "    if total_clamped > 0:\n",
        "        print(f\"      ‚úÖ Fixed {total_clamped} out-of-range values\")\n",
        "    else:\n",
        "        print(\"      ‚úÖ All parameters already within valid ranges\")\n",
        "    \n",
        "    # Fix data types\n",
        "    print(\"   üîß Fixing data types...\")\n",
        "    # Numeric columns should be integers\n",
        "    numeric_cols = [c for c in df_final.columns if c not in ['Game', 'Filename', 'Preset_Name', 'Composer', 'Nationality']]\n",
        "    for col in numeric_cols:\n",
        "        if df_final[col].dtype == 'float64':\n",
        "            df_final[col] = df_final[col].fillna(0).astype('int64')\n",
        "    \n",
        "    # Boolean\n",
        "    if 'Uses_GEMS' in df_final.columns:\n",
        "        df_final['Uses_GEMS'] = df_final['Uses_GEMS'].astype(bool)\n",
        "    \n",
        "    # Save final dataset\n",
        "    print(f\"   üíæ Saving final dataset...\")\n",
        "    df_final.to_csv(FINAL_CSV, index=False)\n",
        "    print(f\"   ‚úÖ Saved {len(df_final):,} presets to {FINAL_CSV}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\nüìä Final Dataset Summary:\")\n",
        "    print(f\"   Total presets: {len(df_final):,}\")\n",
        "    print(f\"   Unique games: {df_final['Game'].nunique():,}\")\n",
        "    print(f\"   GEMS games: {df_final['Uses_GEMS'].sum():,} presets\")\n",
        "    print(f\"   With composer info: {df_final['Composer'].notna().sum():,} presets\")\n",
        "    print(f\"   Columns: {len(df_final.columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\"*60)\n",
        "print(\"üì¶ SUMMARY OF GENERATED FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "files_to_check = [\n",
        "    ('data/processed/all_instruments_final.csv', 'Raw presets extracted from OPM'),\n",
        "    ('artifacts/presets_cleaned.csv', 'Cleaned presets (no TL duplicates per game)'),\n",
        "    ('artifacts/gems_games.csv', 'Games that used GEMS'),\n",
        "    ('artifacts/vgmrips_composers.csv', 'Game-composer mappings (VGMrips)'),\n",
        "    ('artifacts/composers_info.csv', 'Composer info (Wikipedia)'),\n",
        "]\n",
        "\n",
        "for filepath, description in files_to_check:\n",
        "    path = Path(filepath)\n",
        "    if path.exists():\n",
        "        df_temp = pd.read_csv(path)\n",
        "        print(f\"\\n‚úÖ {filepath}\")\n",
        "        print(f\"   üìù {description}\")\n",
        "        print(f\"   üìä Records: {len(df_temp):,}\")\n",
        "        print(f\"   üìã Columns: {list(df_temp.columns)}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå {filepath}\")\n",
        "        print(f\"   üìù {description}\")\n",
        "        print(f\"   ‚ö†Ô∏è File not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ Data extraction complete.\")\n",
        "print(\"   You can now run 02-Data_Analysis.ipynb\")\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 13. Summary and Next Steps\n",
        "\n",
        "### 13.1 What We've Accomplished\n",
        "\n",
        "**Data Extraction:**\n",
        "- ‚úÖ Extracted 93,000+ presets from OPM files\n",
        "- ‚úÖ Parsed 58 parameters per preset\n",
        "- ‚úÖ Extracted game names from filenames\n",
        "- ‚úÖ Removed duplicates (TL-based, per-game)\n",
        "\n",
        "**Data Enrichment:**\n",
        "- ‚úÖ Scraped GEMS game list (200+ games)\n",
        "- ‚úÖ Scraped game-composer mappings from VGMrips\n",
        "- ‚úÖ Extracted composer information from Wikipedia\n",
        "- ‚úÖ Enriched dataset with creative context\n",
        "\n",
        "**Data Quality:**\n",
        "- ‚úÖ Validated parameter ranges\n",
        "- ‚úÖ Checked data completeness\n",
        "- ‚úÖ Normalized game and composer names\n",
        "- ‚úÖ Created consistent data types\n",
        "\n",
        "**Output:**\n",
        "- ‚úÖ Generated cleaned dataset\n",
        "- ‚úÖ Created enrichment files\n",
        "- ‚úÖ Organized in artifacts directory\n",
        "- ‚úÖ Documented all steps\n",
        "\n",
        "### 13.2 The Data Pipeline: From Raw to Analysis-Ready\n",
        "\n",
        "**Complete Pipeline:**\n",
        "\n",
        "```\n",
        "Raw OPM Files (ZIP)\n",
        "    ‚Üì\n",
        "Extract Parameters (Regex)\n",
        "    ‚Üì\n",
        "Extract Game Names (Filename Parsing)\n",
        "    ‚Üì\n",
        "Remove Duplicates (TL-based, per-game)\n",
        "    ‚Üì\n",
        "Enrich with GEMS (Web Scraping)\n",
        "    ‚Üì\n",
        "Enrich with Composers (VGMrips + Wikipedia)\n",
        "    ‚Üì\n",
        "Validate & Clean (Data Quality Checks)\n",
        "    ‚Üì\n",
        "Export to CSV (Artifacts Directory)\n",
        "    ‚Üì\n",
        "Ready for Analysis (Data_Analysis.ipynb)\n",
        "```\n",
        "\n",
        "**Each Step Adds Value:**\n",
        "- **Extraction**: Raw data ‚Üí Structured data\n",
        "- **Cleaning**: Structured data ‚Üí Clean data\n",
        "- **Enrichment**: Clean data ‚Üí Enriched data\n",
        "- **Validation**: Enriched data ‚Üí Analysis-ready data\n",
        "\n",
        "### 13.3 Integration with 02-Data_Analysis.ipynb\n",
        "\n",
        "**Workflow:**\n",
        "1. Run `Data_Extraction.ipynb` first (creates data)\n",
        "2. Run `Data_Analysis.ipynb` second (analyzes data)\n",
        "3. Both notebooks are independent but connected\n",
        "\n",
        "### 13.4 Future Improvements\n",
        "\n",
        "**Potential Enhancements:**\n",
        "- **More Sources**: Additional composer/game databases\n",
        "- **Better Matching**: Improved fuzzy matching algorithms\n",
        "- **Real-Time Updates**: Automatic re-scraping when sources update\n",
        "- **Data Validation**: More comprehensive quality checks\n",
        "- **Performance**: Faster processing for larger datasets\n",
        "\n",
        "**Community Contributions:**\n",
        "- **Manual Mappings**: Community can add known composers\n",
        "- **Corrections**: Fix errors in game/composer names\n",
        "- **New Sources**: Suggest additional data sources\n",
        "- **Improvements**: Better extraction methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**‚úÖ Data extraction complete!**  \n",
        "**üìä All data is ready for analysis in `02-Data_Analysis.ipynb`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" style=\"margin-top: 40px; padding: 20px; border-top: 2px solid #00d4ff;\">\n",
        "\n",
        "### <span style=\"color: #00d4ff\">KASSER</span> <span style=\"color: #ffffff\">SYNTHS</span>\n",
        "\n",
        "*This analysis was created with ‚ù§Ô∏è for the retro gaming and chiptune community.*\n",
        "\n",
        "**Visit us at [kassersynths.com](https://kassersynths.com)**\n",
        "\n",
        "</div>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}